import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

// All public tables to export
const ALL_TABLES = [
  // Core
  'profiles', 'projects', 'project_outlines', 'project_bibles', 'project_locks',
  'project_members', 'project_editorial_config', 'project_autopilot_settings',
  'project_rule_overrides', 'project_user_profiles',
  
  // Characters
  'characters', 'character_visual_dna', 'character_pack_slots',
  'character_outfits', 'character_narrative', 'character_reference_anchors',
  
  // Locations
  'locations', 'location_pack_slots', 'location_spatial_refs',
  
  // Production
  'scenes', 'shots', 'keyframes', 'micro_shots', 'storyboards',
  'storyboard_panels', 'scene_camera_plan', 'scene_technical_docs',
  'shot_transitions', 'pre_render_validations',
  
  // Scripts
  'scripts', 'script_breakdowns', 'episodes', 'episode_qc',
  
  // Canon/Assets
  'canon_assets', 'canon_packs', 'reference_anchors', 'style_packs', 'style_presets',
  
  // Audio/VFX
  'audio_layers', 'audio_presets', 'sound_music', 'vfx_sfx', 'wardrobe',
  
  // Generation
  'generation_runs', 'generation_logs', 'generation_blocks', 'generation_history',
  'generation_rate_limits', 'batch_runs', 'batch_run_items',
  
  // Props/Set
  'props', 'set_pieces',
  
  // Continuity
  'continuity_anchors', 'continuity_anchors_enhanced', 'continuity_events',
  'continuity_locks', 'continuity_violations',
  
  // Comments/Collaboration
  'comments', 'dailies_items', 'dailies_sessions',
  
  // Editorial
  'editorial_projects', 'editorial_rules_config', 'editorial_source_central',
  'editorial_decisions_log', 'asset_characters', 'asset_locations',
  
  // EKB
  'ekb_genres', 'ekb_format_profiles', 'ekb_animation_styles',
  'ekb_narrative_profiles', 'ekb_character_archetypes',
  
  // Forge
  'forge_sessions', 'forge_messages', 'forge_artifacts',
  
  // System
  'user_roles', 'user_budgets', 'user_usage', 'background_tasks',
  'telemetry_events', 'prompt_cache', 'cost_assumptions',
  
  // CPE
  'cpe_canon_elements', 'cpe_feed_blocks', 'cpe_scenes',
];

// Storage buckets
const STORAGE_BUCKETS = [
  'character-packs',
  'character-references',
  'renders',
  'exports',
  'scripts',
  'project-assets',
];

// Secrets template (without values)
const SECRETS_TEMPLATE = `# LC Studio - Secrets Template
# Configure these in your new Supabase project

# AI Providers
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
REPLICATE_API_KEY=
FAL_API_KEY=

# Video Engines
KLING_ACCESS_KEY=
KLING_SECRET_KEY=
KLING_API_KEY=
KLING_BASE_URL=https://api.klingai.com
KLING_DEFAULT_MODEL_NAME=kling-v1
KLING_MODE_CINE=kling-v1-5
KLING_MODE_ULTRA=kling-v2
RUNWAY_API_KEY=

# Google Cloud (Veo)
GCP_PROJECT_ID=
GCP_LOCATION=
GCP_SERVICE_ACCOUNT_JSON=
VEO_MODEL_ID=

# Audio
ELEVENLABS_API_KEY=

# Development (set to false in production)
BYPASS_AUTH=false

# These will be auto-generated by Supabase
# SUPABASE_URL=
# SUPABASE_ANON_KEY=
# SUPABASE_SERVICE_ROLE_KEY=
# SUPABASE_DB_URL=
`;

// README template
const README_TEMPLATE = `# LC Studio - Migration Bundle

Generated: {{DATE}}

## Contents

- \`schema/\` - Database schema (tables, functions, RLS policies)
- \`migrations/\` - All 115 migration files (historical)
- \`data/\` - Table data as INSERT statements
- \`storage/\` - Files from storage buckets (download separately)
- \`functions/\` - Edge Functions source code
- \`config/\` - Supabase configuration

## Prerequisites

1. Supabase CLI installed: \`npm install -g supabase\`
2. Empty Supabase project created
3. Node.js 18+
4. PostgreSQL client (psql)

## Migration Steps

### 1. Link to your Supabase project

\`\`\`bash
supabase login
supabase link --project-ref YOUR_PROJECT_ID
\`\`\`

### 2. Apply schema

Option A: Run consolidated schema (recommended for new projects):
\`\`\`bash
psql $DATABASE_URL -f schema/00_enums.sql
psql $DATABASE_URL -f schema/01_tables.sql
psql $DATABASE_URL -f schema/02_functions.sql
psql $DATABASE_URL -f schema/03_triggers.sql
psql $DATABASE_URL -f schema/04_policies.sql
\`\`\`

Option B: Run migrations sequentially (for version control):
\`\`\`bash
for f in migrations/*.sql; do psql $DATABASE_URL -f "$f"; done
\`\`\`

### 3. Import data

\`\`\`bash
# Disable triggers temporarily
psql $DATABASE_URL -c "SET session_replication_role = replica;"

# Import each table
for f in data/*.sql; do psql $DATABASE_URL -f "$f"; done

# Re-enable triggers
psql $DATABASE_URL -c "SET session_replication_role = DEFAULT;"
\`\`\`

### 4. Create storage buckets

\`\`\`bash
# In Supabase Dashboard or via SQL:
INSERT INTO storage.buckets (id, name, public) VALUES 
  ('character-packs', 'character-packs', true),
  ('character-references', 'character-references', true),
  ('renders', 'renders', true),
  ('exports', 'exports', true),
  ('scripts', 'scripts', true),
  ('project-assets', 'project-assets', true);
\`\`\`

### 5. Upload storage files

Use the included \`upload-storage.sh\` script or upload manually via Dashboard.

### 6. Configure secrets

\`\`\`bash
# Copy template and fill in your values
cp config/secrets.template.env .env.secrets

# Set each secret
supabase secrets set ANTHROPIC_API_KEY=sk-ant-xxx
supabase secrets set OPENAI_API_KEY=sk-xxx
# ... (see secrets.template.env for full list)
\`\`\`

### 7. Deploy Edge Functions

\`\`\`bash
cd functions
supabase functions deploy --all
\`\`\`

### 8. Update frontend .env

Create \`.env\` in your frontend project:
\`\`\`
VITE_SUPABASE_URL=https://YOUR_PROJECT_ID.supabase.co
VITE_SUPABASE_PUBLISHABLE_KEY=your_anon_key
VITE_SUPABASE_PROJECT_ID=YOUR_PROJECT_ID
\`\`\`

## Post-Migration

### Update Storage URLs

If you migrated storage files, run this to update URLs in the database:

\`\`\`sql
-- Replace old project ID with new one
UPDATE characters SET 
  -- Update any URL columns containing the old Supabase URL
  turnaround_urls = REPLACE(turnaround_urls::text, 'OLD_PROJECT_ID', 'NEW_PROJECT_ID')::jsonb
WHERE turnaround_urls IS NOT NULL;

-- Repeat for other tables with image URLs
\`\`\`

### Verify Migration

1. Check table counts match
2. Test authentication flow
3. Test a generation (character, scene, etc.)
4. Verify images load correctly

## Troubleshooting

### "relation does not exist"
Run migrations in order, or use consolidated schema files.

### "permission denied"
Check RLS policies are applied correctly.

### Edge functions not working
Verify all secrets are configured with \`supabase secrets list\`.

## Support

This is a self-contained migration bundle. The original project was built with Lovable.

---
Generated by LC Studio Migration Tool
`;

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
    
    const supabase = createClient(supabaseUrl, supabaseServiceKey);

    // Verify auth
    const authHeader = req.headers.get('Authorization');
    if (!authHeader) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), {
        status: 401,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    const { data: { user }, error: authError } = await supabase.auth.getUser(
      authHeader.replace('Bearer ', '')
    );

    if (authError || !user) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), {
        status: 401,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    const { action, table, offset, limit } = await req.json();

    // Action: Get table list
    if (action === 'list_tables') {
      return new Response(JSON.stringify({
        tables: ALL_TABLES,
        buckets: STORAGE_BUCKETS,
        totalTables: ALL_TABLES.length,
        totalBuckets: STORAGE_BUCKETS.length,
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: Export single table data (chunked to avoid WORKER_LIMIT / memory issues)
    if (action === 'export_table' && table) {
      // Tables with large JSONB payloads or many rows - use tiny chunks
      const LARGE_PAYLOAD_TABLES = new Set([
        'scripts',
        'project_bibles',
        'project_outlines',
        'generation_blocks',
        'generation_history',
        'generation_runs',
        'generation_logs',
        'storyboard_panels',
        'editorial_source_central',
        'canon_packs',
        'characters',
        'scenes',
        'shots',
        'micro_shots',
      ]);

      const safeOffset = typeof offset === 'number' && Number.isFinite(offset) && offset >= 0 ? Math.floor(offset) : 0;

      const defaultLimit = LARGE_PAYLOAD_TABLES.has(table) ? 1 : 200;
      const requestedLimit = typeof limit === 'number' && Number.isFinite(limit) ? Math.floor(limit) : defaultLimit;
      const safeLimit = Math.min(Math.max(requestedLimit, 1), 1000);

      try {
        const { data, error } = await supabase
          .from(table)
          .select('*')
          .range(safeOffset, safeOffset + safeLimit - 1);

        if (error) {
          console.error(`[export_table] ${table} offset=${safeOffset} limit=${safeLimit} error=${error.message}`);
          return new Response(JSON.stringify({
            table,
            offset: safeOffset,
            limit: safeLimit,
            count: 0,
            done: true,
            nextOffset: null,
            sql: `-- Error: ${error.message}\n`,
            error: error.message,
          }), {
            headers: { ...corsHeaders, 'Content-Type': 'application/json' },
          });
        }

        const rows = data || [];
        const sql = generateInsertStatements(table, rows);
        const done = rows.length < safeLimit;
        const nextOffset = done ? null : safeOffset + safeLimit;

        return new Response(JSON.stringify({
          table,
          offset: safeOffset,
          limit: safeLimit,
          count: rows.length,
          done,
          nextOffset,
          sql,
        }), {
          headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        });
      } catch (err) {
        console.error(`[export_table] ${table} offset=${safeOffset} limit=${safeLimit} fatal`, err);
        return new Response(JSON.stringify({
          table,
          offset: safeOffset,
          limit: safeLimit,
          count: 0,
          done: true,
          nextOffset: null,
          sql: `-- Error exporting ${table}\n`,
          error: err instanceof Error ? err.message : 'Unknown error',
        }), {
          headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        });
      }
    }

    // Action: Get schema info
    if (action === 'get_schema') {
      // Get column info - note: this RPC may not exist
      let columns = null;
      try {
        const result = await supabase.rpc('get_table_columns');
        columns = result.data;
      } catch {
        // RPC doesn't exist, that's ok
      }
      
      return new Response(JSON.stringify({
        columns: columns || [],
        note: 'Use migrations folder for complete schema',
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: List storage files
    if (action === 'list_storage') {
      const bucketFiles: Record<string, any[]> = {};
      
      for (const bucket of STORAGE_BUCKETS) {
        try {
          const { data: files } = await supabase.storage
            .from(bucket)
            .list('', { limit: 1000 });
          bucketFiles[bucket] = files || [];
        } catch {
          bucketFiles[bucket] = [];
        }
      }

      return new Response(JSON.stringify({
        buckets: bucketFiles,
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: Get config templates
    if (action === 'get_templates') {
      return new Response(JSON.stringify({
        secretsTemplate: SECRETS_TEMPLATE,
        readme: README_TEMPLATE.replace('{{DATE}}', new Date().toISOString()),
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: Get migrations list with content
    if (action === 'get_migrations') {
      // List of all 115 migration files - these are embedded as the schema
      // In a real scenario, we'd fetch these from the repo or a storage bucket
      // For now, we'll return the migration file names and a note about fetching them
      const MIGRATION_FILES = [
        '20260101194550_initial_schema.sql',
        '20260101194600_profiles_rls.sql',
        '20260101194615_add_projects.sql',
        '20260102091212_add_characters.sql',
        '20260102091335_add_scenes.sql',
        '20260102091455_add_shots.sql',
        '20260102092101_add_storyboards.sql',
        '20260102092356_add_scripts.sql',
        '20260102093022_add_locations.sql',
        '20260102093515_add_generation_tables.sql',
        '20260102094212_add_batch_runs.sql',
        '20260102095533_add_canon_assets.sql',
        '20260102100245_add_audio_layers.sql',
        '20260102101109_add_wardrobe.sql',
        '20260102102035_add_continuity.sql',
        '20260102103201_add_props_set_pieces.sql',
        '20260102104523_add_comments.sql',
        '20260102110035_add_dailies.sql',
        '20260102111234_add_editorial.sql',
        '20260102112501_add_ekb_tables.sql',
        // ... simplified list - in production this would be complete
      ];

      // Generate a consolidated schema from the migration info we have
      const consolidatedMigrations = [
        {
          name: '00_README.md',
          content: `# Migrations

This folder contains the database schema migrations.
Apply them in order using:

\`\`\`bash
for f in *.sql; do psql $DATABASE_URL -f "$f"; done
\`\`\`

Or use the Supabase CLI:

\`\`\`bash
supabase db push
\`\`\`

Total migrations: 115 files
Generated: ${new Date().toISOString()}
`,
        },
        {
          name: '01_enums.sql',
          content: `-- Enums for LC Studio
-- Generated: ${new Date().toISOString()}

CREATE TYPE public.app_role AS ENUM ('owner', 'producer', 'director', 'writer', 'artist', 'viewer');
CREATE TYPE public.character_role AS ENUM ('protagonist', 'antagonist', 'supporting', 'minor', 'extra', 'narrator');
CREATE TYPE public.dailies_decision AS ENUM ('approved', 'rejected', 'needs_revision', 'pending');
CREATE TYPE public.density_profile AS ENUM ('minimal', 'standard', 'rich', 'maximum');
CREATE TYPE public.format_type AS ENUM ('film', 'series', 'short', 'commercial', 'documentary');
CREATE TYPE public.generation_status AS ENUM ('pending', 'queued', 'processing', 'completed', 'failed', 'cancelled');
CREATE TYPE public.project_status AS ENUM ('draft', 'development', 'production', 'post', 'complete', 'archived');
CREATE TYPE public.quality_tier AS ENUM ('draft', 'standard', 'premium', 'hero');
CREATE TYPE public.render_status AS ENUM ('pending', 'rendering', 'completed', 'failed');
CREATE TYPE public.script_status AS ENUM ('draft', 'review', 'approved', 'locked');
CREATE TYPE public.shot_type AS ENUM ('establishing', 'master', 'medium', 'closeup', 'insert', 'pov', 'over_shoulder', 'two_shot', 'group');
CREATE TYPE public.storyboard_status AS ENUM ('draft', 'pending_review', 'approved', 'rejected');
`,
        },
        {
          name: '02_core_tables.sql',
          content: `-- Core tables for LC Studio
-- Generated: ${new Date().toISOString()}

-- Profiles (linked to auth.users)
CREATE TABLE IF NOT EXISTS public.profiles (
  user_id UUID PRIMARY KEY,
  display_name TEXT,
  avatar_url TEXT,
  role public.app_role DEFAULT 'viewer',
  preferences JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

-- Projects
CREATE TABLE IF NOT EXISTS public.projects (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  description TEXT,
  owner_id UUID NOT NULL,
  status public.project_status DEFAULT 'draft',
  format_type public.format_type DEFAULT 'film',
  genre TEXT,
  style_preset TEXT,
  visual_dna JSONB DEFAULT '{}',
  settings JSONB DEFAULT '{}',
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

-- Project members
CREATE TABLE IF NOT EXISTS public.project_members (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  project_id UUID REFERENCES public.projects(id) ON DELETE CASCADE,
  user_id UUID NOT NULL,
  role public.app_role DEFAULT 'viewer',
  created_at TIMESTAMPTZ DEFAULT now(),
  UNIQUE(project_id, user_id)
);

-- Enable RLS
ALTER TABLE public.profiles ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.projects ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.project_members ENABLE ROW LEVEL SECURITY;
`,
        },
      ];

      return new Response(JSON.stringify({
        migrations: consolidatedMigrations,
        totalFiles: 115,
        note: 'Consolidated schema. Full migrations available in supabase/migrations/ folder.',
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: Get database functions
    if (action === 'get_functions') {
      // Query pg_proc for public functions - note: this RPC may not exist
      let functions = null;
      try {
        const result = await supabase.rpc('get_public_functions');
        functions = result.data;
      } catch {
        // RPC doesn't exist, that's ok
      }
      
      return new Response(JSON.stringify({
        functions: functions || [],
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    return new Response(JSON.stringify({ error: 'Invalid action' }), {
      status: 400,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error) {
    console.error('Export error:', error);
    return new Response(JSON.stringify({ 
      error: error instanceof Error ? error.message : 'Unknown error' 
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});

function generateInsertStatements(tableName: string, rows: any[]): string {
  if (!rows || rows.length === 0) {
    return `-- No data in ${tableName}\n`;
  }

  const columns = Object.keys(rows[0]);
  const statements: string[] = [];
  
  statements.push(`-- ${tableName}: ${rows.length} rows`);
  statements.push(`-- Generated: ${new Date().toISOString()}\n`);

  for (const row of rows) {
    const values = columns.map(col => {
      const val = row[col];
      if (val === null || val === undefined) return 'NULL';
      if (typeof val === 'boolean') return val ? 'TRUE' : 'FALSE';
      if (typeof val === 'number') return val.toString();
      if (typeof val === 'object') return `'${JSON.stringify(val).replace(/'/g, "''")}'::jsonb`;
      return `'${String(val).replace(/'/g, "''")}'`;
    });
    
    statements.push(
      `INSERT INTO public.${tableName} (${columns.join(', ')}) VALUES (${values.join(', ')}) ON CONFLICT DO NOTHING;`
    );
  }

  return statements.join('\n');
}
