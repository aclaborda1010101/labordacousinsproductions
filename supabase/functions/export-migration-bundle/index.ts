import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

// All public tables to export
const ALL_TABLES = [
  // Core
  'profiles', 'projects', 'project_outlines', 'project_bibles', 'project_locks',
  'project_members', 'project_editorial_config', 'project_autopilot_settings',
  'project_rule_overrides', 'project_user_profiles',
  
  // Characters
  'characters', 'character_visual_dna', 'character_pack_slots',
  'character_outfits', 'character_narrative', 'character_reference_anchors',
  
  // Locations
  'locations', 'location_pack_slots', 'location_spatial_refs',
  
  // Production
  'scenes', 'shots', 'keyframes', 'micro_shots', 'storyboards',
  'storyboard_panels', 'scene_camera_plan', 'scene_technical_docs',
  'shot_transitions', 'pre_render_validations',
  
  // Scripts
  'scripts', 'script_breakdowns', 'episodes', 'episode_qc',
  
  // Canon/Assets
  'canon_assets', 'canon_packs', 'reference_anchors', 'style_packs', 'style_presets',
  
  // Audio/VFX
  'audio_layers', 'audio_presets', 'sound_music', 'vfx_sfx', 'wardrobe',
  
  // Generation
  'generation_runs', 'generation_logs', 'generation_blocks', 'generation_history',
  'generation_rate_limits', 'batch_runs', 'batch_run_items',
  
  // Props/Set
  'props', 'set_pieces',
  
  // Continuity
  'continuity_anchors', 'continuity_anchors_enhanced', 'continuity_events',
  'continuity_locks', 'continuity_violations',
  
  // Comments/Collaboration
  'comments', 'dailies_items', 'dailies_sessions',
  
  // Editorial
  'editorial_projects', 'editorial_rules_config', 'editorial_source_central',
  'editorial_decisions_log', 'asset_characters', 'asset_locations',
  
  // EKB
  'ekb_genres', 'ekb_format_profiles', 'ekb_animation_styles',
  'ekb_narrative_profiles', 'ekb_character_archetypes',
  
  // Forge
  'forge_sessions', 'forge_messages', 'forge_artifacts',
  
  // System
  'user_roles', 'user_budgets', 'user_usage', 'background_tasks',
  'telemetry_events', 'prompt_cache', 'cost_assumptions',
  
  // CPE
  'cpe_canon_elements', 'cpe_feed_blocks', 'cpe_scenes',
];

// Storage buckets
const STORAGE_BUCKETS = [
  'character-packs',
  'character-references',
  'renders',
  'exports',
  'scripts',
  'project-assets',
];

// Secrets template (without values)
const SECRETS_TEMPLATE = `# LC Studio - Secrets Template
# Configure these in your new Supabase project

# AI Providers
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
REPLICATE_API_KEY=
FAL_API_KEY=

# Video Engines
KLING_ACCESS_KEY=
KLING_SECRET_KEY=
KLING_API_KEY=
KLING_BASE_URL=https://api.klingai.com
KLING_DEFAULT_MODEL_NAME=kling-v1
KLING_MODE_CINE=kling-v1-5
KLING_MODE_ULTRA=kling-v2
RUNWAY_API_KEY=

# Google Cloud (Veo)
GCP_PROJECT_ID=
GCP_LOCATION=
GCP_SERVICE_ACCOUNT_JSON=
VEO_MODEL_ID=

# Audio
ELEVENLABS_API_KEY=

# Development (set to false in production)
BYPASS_AUTH=false

# These will be auto-generated by Supabase
# SUPABASE_URL=
# SUPABASE_ANON_KEY=
# SUPABASE_SERVICE_ROLE_KEY=
# SUPABASE_DB_URL=
`;

// README template
const README_TEMPLATE = `# LC Studio - Migration Bundle

Generated: {{DATE}}

## Contents

- \`schema/\` - Database schema (tables, functions, RLS policies)
- \`migrations/\` - All 115 migration files (historical)
- \`data/\` - Table data as INSERT statements
- \`storage/\` - Files from storage buckets (download separately)
- \`functions/\` - Edge Functions source code
- \`config/\` - Supabase configuration

## Prerequisites

1. Supabase CLI installed: \`npm install -g supabase\`
2. Empty Supabase project created
3. Node.js 18+
4. PostgreSQL client (psql)

## Migration Steps

### 1. Link to your Supabase project

\`\`\`bash
supabase login
supabase link --project-ref YOUR_PROJECT_ID
\`\`\`

### 2. Apply schema

Option A: Run consolidated schema (recommended for new projects):
\`\`\`bash
psql $DATABASE_URL -f schema/00_enums.sql
psql $DATABASE_URL -f schema/01_tables.sql
psql $DATABASE_URL -f schema/02_functions.sql
psql $DATABASE_URL -f schema/03_triggers.sql
psql $DATABASE_URL -f schema/04_policies.sql
\`\`\`

Option B: Run migrations sequentially (for version control):
\`\`\`bash
for f in migrations/*.sql; do psql $DATABASE_URL -f "$f"; done
\`\`\`

### 3. Import data

\`\`\`bash
# Disable triggers temporarily
psql $DATABASE_URL -c "SET session_replication_role = replica;"

# Import each table
for f in data/*.sql; do psql $DATABASE_URL -f "$f"; done

# Re-enable triggers
psql $DATABASE_URL -c "SET session_replication_role = DEFAULT;"
\`\`\`

### 4. Create storage buckets

\`\`\`bash
# In Supabase Dashboard or via SQL:
INSERT INTO storage.buckets (id, name, public) VALUES 
  ('character-packs', 'character-packs', true),
  ('character-references', 'character-references', true),
  ('renders', 'renders', true),
  ('exports', 'exports', true),
  ('scripts', 'scripts', true),
  ('project-assets', 'project-assets', true);
\`\`\`

### 5. Upload storage files

Use the included \`upload-storage.sh\` script or upload manually via Dashboard.

### 6. Configure secrets

\`\`\`bash
# Copy template and fill in your values
cp config/secrets.template.env .env.secrets

# Set each secret
supabase secrets set ANTHROPIC_API_KEY=sk-ant-xxx
supabase secrets set OPENAI_API_KEY=sk-xxx
# ... (see secrets.template.env for full list)
\`\`\`

### 7. Deploy Edge Functions

\`\`\`bash
cd functions
supabase functions deploy --all
\`\`\`

### 8. Update frontend .env

Create \`.env\` in your frontend project:
\`\`\`
VITE_SUPABASE_URL=https://YOUR_PROJECT_ID.supabase.co
VITE_SUPABASE_PUBLISHABLE_KEY=your_anon_key
VITE_SUPABASE_PROJECT_ID=YOUR_PROJECT_ID
\`\`\`

## Post-Migration

### Update Storage URLs

If you migrated storage files, run this to update URLs in the database:

\`\`\`sql
-- Replace old project ID with new one
UPDATE characters SET 
  -- Update any URL columns containing the old Supabase URL
  turnaround_urls = REPLACE(turnaround_urls::text, 'OLD_PROJECT_ID', 'NEW_PROJECT_ID')::jsonb
WHERE turnaround_urls IS NOT NULL;

-- Repeat for other tables with image URLs
\`\`\`

### Verify Migration

1. Check table counts match
2. Test authentication flow
3. Test a generation (character, scene, etc.)
4. Verify images load correctly

## Troubleshooting

### "relation does not exist"
Run migrations in order, or use consolidated schema files.

### "permission denied"
Check RLS policies are applied correctly.

### Edge functions not working
Verify all secrets are configured with \`supabase secrets list\`.

## Support

This is a self-contained migration bundle. The original project was built with Lovable.

---
Generated by LC Studio Migration Tool
`;

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
    
    const supabase = createClient(supabaseUrl, supabaseServiceKey);

    // Verify auth
    const authHeader = req.headers.get('Authorization');
    if (!authHeader) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), {
        status: 401,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    const { data: { user }, error: authError } = await supabase.auth.getUser(
      authHeader.replace('Bearer ', '')
    );

    if (authError || !user) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), {
        status: 401,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    const { action, table } = await req.json();

    // Action: Get table list
    if (action === 'list_tables') {
      return new Response(JSON.stringify({
        tables: ALL_TABLES,
        buckets: STORAGE_BUCKETS,
        totalTables: ALL_TABLES.length,
        totalBuckets: STORAGE_BUCKETS.length,
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: Export single table data
    if (action === 'export_table' && table) {
      const { data, error } = await supabase
        .from(table)
        .select('*')
        .limit(50000);

      if (error) {
        // Table might not exist or be empty
        return new Response(JSON.stringify({
          table,
          data: [],
          count: 0,
          error: error.message,
        }), {
          headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        });
      }

      // Generate INSERT statements
      const inserts = generateInsertStatements(table, data || []);

      return new Response(JSON.stringify({
        table,
        data: data || [],
        count: data?.length || 0,
        sql: inserts,
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: Get schema info
    if (action === 'get_schema') {
      // Get column info - note: this RPC may not exist
      let columns = null;
      try {
        const result = await supabase.rpc('get_table_columns');
        columns = result.data;
      } catch {
        // RPC doesn't exist, that's ok
      }
      
      return new Response(JSON.stringify({
        columns: columns || [],
        note: 'Use migrations folder for complete schema',
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: List storage files
    if (action === 'list_storage') {
      const bucketFiles: Record<string, any[]> = {};
      
      for (const bucket of STORAGE_BUCKETS) {
        try {
          const { data: files } = await supabase.storage
            .from(bucket)
            .list('', { limit: 1000 });
          bucketFiles[bucket] = files || [];
        } catch {
          bucketFiles[bucket] = [];
        }
      }

      return new Response(JSON.stringify({
        buckets: bucketFiles,
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: Get config templates
    if (action === 'get_templates') {
      return new Response(JSON.stringify({
        secretsTemplate: SECRETS_TEMPLATE,
        readme: README_TEMPLATE.replace('{{DATE}}', new Date().toISOString()),
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    // Action: Get database functions
    if (action === 'get_functions') {
      // Query pg_proc for public functions - note: this RPC may not exist
      let functions = null;
      try {
        const result = await supabase.rpc('get_public_functions');
        functions = result.data;
      } catch {
        // RPC doesn't exist, that's ok
      }
      
      return new Response(JSON.stringify({
        functions: functions || [],
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      });
    }

    return new Response(JSON.stringify({ error: 'Invalid action' }), {
      status: 400,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error) {
    console.error('Export error:', error);
    return new Response(JSON.stringify({ 
      error: error instanceof Error ? error.message : 'Unknown error' 
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});

function generateInsertStatements(tableName: string, rows: any[]): string {
  if (!rows || rows.length === 0) {
    return `-- No data in ${tableName}\n`;
  }

  const columns = Object.keys(rows[0]);
  const statements: string[] = [];
  
  statements.push(`-- ${tableName}: ${rows.length} rows`);
  statements.push(`-- Generated: ${new Date().toISOString()}\n`);

  for (const row of rows) {
    const values = columns.map(col => {
      const val = row[col];
      if (val === null || val === undefined) return 'NULL';
      if (typeof val === 'boolean') return val ? 'TRUE' : 'FALSE';
      if (typeof val === 'number') return val.toString();
      if (typeof val === 'object') return `'${JSON.stringify(val).replace(/'/g, "''")}'::jsonb`;
      return `'${String(val).replace(/'/g, "''")}'`;
    });
    
    statements.push(
      `INSERT INTO public.${tableName} (${columns.join(', ')}) VALUES (${values.join(', ')}) ON CONFLICT DO NOTHING;`
    );
  }

  return statements.join('\n');
}
